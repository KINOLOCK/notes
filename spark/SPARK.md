# SPARK

## 1. Introduction to Data Analysis with Spark
----------------------------------------------------------------------------

**Apache Spark** is a *cluster computing platform* designed to be fast and general-purpose.
Spark extends the popular **MapReduce** model to efficiently support more types of computations,
including interactive queries and stream processing.
Spark can run in **Hadoop clusters** and access any Hadoop data source, including **Cassandra**.

### Spark Core
Spark Core contains the basic functionality of Spark, and the API that defines **resilient distributed data sets (RDDs)**,
which are Spark’s main programming abstraction.
RDDs represent a collection of items **distributed across many compute nodes** that can be **manipulated in parallel**.

### Spark SQL
Spark SQL is Spark’s package for working with structured data. 
It allows **querying data via SQL** as well as the Apache Hive variant of SQL,
called the Hive Query Language (HQL) — and it supports many sources of data, including Hive tables, Parquet, and JSON.
Beyond providing a SQL interface to Spark, Spark SQL allows developers
to intermix SQL queries with the programmatic data manipulations supported by
RDDs in Python, Java, and Scala, all within a single application, thus combining SQL
with complex analytics.

### Spark Streaming
Spark Streaming is a Spark component that enables processing of **live streams of data**.
Examples of data streams include logfiles generated by production web servers, or
queues of messages containing status updates posted by users of a web service.

### MLlib
Spark comes with a library containing common **machine learning (ML)** functionality, called MLlib.
MLlib provides multiple types of machine learning algorithms, including classification, regression, 
clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import.
It also provides some lower-level ML primitives, including a generic gradient descent optimization
algorithm. All of these methods are designed to scale out across a cluster.

### GraphX
GraphX is a library for manipulating **graphs** (for example, a facebook’s friend graph)
and performing graph-parallel computations. 
Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph 
with arbitrary properties attached to each vertex and edge.
GraphX also provides various operators for manipulating graphs (for example, subgraph and mapVertices) and a library of
common graph algorithms (e.g., PageRank and triangle counting).

### Cluster Managers
Under the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes.
To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, 
including Hadoop YARN, Apache Mesos, and **Standalone Scheduler** (included in Spark, best for beginning).

### Who uses Spark?
Spark is used in roughly two aspects:
- Data analytics (mathematicians, statistics, MatLab, R etc.)
- Data processing (programmers, engineers, bussiness apps)

### Storage Layers for Spark
Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS)
or other storage systems supported by the Hadoop APIs (including your local filesystem, Amazon S3, Cassandra, Hive, HBase, etc.).
**Spark does not require Hadoop**; it simply has support for storage systems implementing the Hadoop APIs.
Spark supports text files, SequenceFiles, Avro, Parquet, and any other Hadoop InputFormat(Chapter 5).

## 2. Downloading Spark and Getting Started
----------------------------------------------------------------------------

Spark is written in Scala, and runs on the Java Virtual Machine (JVM), so you need [JDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html).
Go to [Spark downloads](http://spark.apache.org/downloads.html) and select the package type of 
“Pre-built for Hadoop 2.x and later”, click Direct downlad.
Extract the contents to `C:/spark` (no spaces please!).

The `bin` folder contains executables for interacting with Spark, for example the **Spark shell**.
See `examples` for examples, of course...

All of the work in this chapter will be with Spark running in **local mode**,
that is, nondistributed mode, which uses only a single machine.

Although Spark does not require Hadoop to be installed on your machine,
most of the tutorials assume it, so it would be better to install it from Apache's [site](http://hadoop.apache.org/releases.html).
Make sure that the Spark's and Hadoop's version match.

### Spark shell
Spark’s shells allow you to interact with data that is distributed on disk or in memory across many machines,
and Spark takes care of automatically distributing this processing.

Open the `bin/spark-shell` in your CMD.
In `conf` folder you can copy the `log4j.properties.template` and rename it to `log4j.properties`,
to tweak the logging settings. For example, to show only WARN messages in console,
change the `log4j.rootCategory=INFO, console` to `log4j.rootCategory=WARN, console`.

In Spark, we express our computation through operations on RDDs.
RDDs are distributed collections that are automatically parallelized across the cluster.
(**RDD** = **R**esilient **D**istributed **D**atasets)

An example is worth a thousand words, let's count the lines from a file.

```shell
scala> val lines = sc.textFile("README.md") // Create an RDD called lines
lines: spark.RDD[String] = MappedRDD[...]
scala> lines.count() // Count the number of items in this RDD
res0: Long = 95
scala> lines.first() // First item in this RDD, i.e. first line of README.md
res1: String = # Apache Spark
```

**NOTE: The file location is relative to the directory where the console is instantiated!**

To exit shell, press `Ctrl` + `D`.


### Core Spark Concepts
At a high level, every Spark application consists of a **driver program** that launches
various parallel operations on a cluster. The driver program contains your application’s 
main function and defines distributed datasets on the cluster, then applies operations to them.
In previous examples, the driver program was the Spark shell itself,
and you could just type in the operations you wanted to run.

Driver programs access Spark through a **SparkContext object**, which represents a connection to a computing cluster.
In the shell, a SparkContext is automatically created for you as the variable called `sc`. 

RDDs are similar to collections in Scala.
You can call various methods on them, like `filter` for example.
To count only lines that contain the word "Scala", you could write either of these:

```shell
scala> lines.filter(line => line.contains("Scala")).count
res2: Long = 3
scala> lines.filter(_.contains("Scala")).count
res3: Long = 3
```

### Standalone Applications
The main difference from using it in the shell is that you need to initialize your own SparkContext.
After that, the API is the same.

```scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)
```

This is the minimalistic way to initialize a SparkContext, where you pass two parameters:
- A **cluster URL**, "local" in these examples, which tells Spark how to connect to a cluster.
The value "local" is a **special value** that runs Spark on one thread on the local machine, **without connecting to a cluster**.
- An **application name**, namely "My App" in examples.
This will identify your application on the cluster manager’s UI if you connect to a cluster.

Of course, there are additional settings that you can pass.
To shut down Spark, you can call the `stop()` method on your SparkContext,
or just exit the application...

### Sample app, word count

On a single machine, implementing word count is simple, but in distributed frameworks it is a common example,
because it involves reading and combining data from many worker nodes.
You need to add Spark dependency in your build (sbt):

```scala
name := """hello-spark"""

version := "1.0"

scalaVersion := "2.11.7"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.1" % "provided"
```

The spark-core package is marked as `provided` in case we package our application into an assembly JAR.

```scala
// Create a Spark Context.
val conf = new SparkConf().setAppName("wordCount")
val sc = new SparkContext(conf)
// Load our input data.
val input = sc.textFile(inputFile)
// Split it up into words.
val words = input.flatMap(line => line.split(" "))
// Transform into pairs and count.
val counts = words.map(word => (word, 1)).reduceByKey{case (x, y) => x + y}
// Save the word count out to a text file, causing evaluation.
counts.saveAsTextFile(outputFile)
```

You should get more than one file as the output of `saveAsTextFile()` method.
Later we'll see why.




















# FAQ, errors
---------------------------------------------

- java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
http://teknosrc.com/spark-error-java-io-ioexception-could-not-locate-executable-null-bin-winutils-exe-hadoop-binaries/
(Install Hadoop and download winutils.exe, put it in Hadoop's bin folder)






